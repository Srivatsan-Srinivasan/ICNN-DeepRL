{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../src\")\n",
    "import tensorflow as tf\n",
    "\n",
    "import gym\n",
    "import baselines\n",
    "from baselines import deepq\n",
    "\n",
    "import normalized_env\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def modify_env(env):\n",
    "\n",
    "    # hack to give us control over the initial state\n",
    "    import types\n",
    "\n",
    "    def new_timelimt_step(self, action):\n",
    "        assert self._episode_started_at is not None, \"Cannot call env.step() before calling reset()\"\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        self._elapsed_steps += 1\n",
    "\n",
    "        if self._past_limit():\n",
    "            if self.metadata.get('semantics.autoreset'):\n",
    "                _ = self.reset() # automatically reset the env\n",
    "            done = True \n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def new_step(self, discrete_action):\n",
    "        disc_to_cont = {\n",
    "            0: [-1.0],\n",
    "            1: [-0.5],\n",
    "            2: [0.0],\n",
    "            3: [0.5],\n",
    "            4: [1.0]\n",
    "        }\n",
    "\n",
    "        action = disc_to_cont[int(discrete_action[0])]\n",
    "        # hacky way to map continuous action to discrete value\n",
    "\n",
    "        position = self.state[0]\n",
    "        velocity = self.state[1]\n",
    "        force = min(max(action[0], -1.0), 1.0)\n",
    "\n",
    "        velocity += force*self.power -0.0025 * math.cos(3*position)\n",
    "        if (velocity > self.max_speed): velocity = self.max_speed\n",
    "        if (velocity < -self.max_speed): velocity = -self.max_speed\n",
    "        position += velocity\n",
    "        if (position > self.max_position): position = self.max_position\n",
    "        if (position < self.min_position): position = self.min_position\n",
    "        if (position==self.min_position and velocity<0): velocity = 0\n",
    "\n",
    "        done = bool(position >= self.goal_position)\n",
    "\n",
    "        reward = 0\n",
    "        if done:\n",
    "            reward = 100.0\n",
    "        reward-= math.pow(action[0],2)*0.1\n",
    "\n",
    "        self.state = np.array([position, velocity])\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "    # set max time step\n",
    "    \n",
    "    env._max_episode_steps = 999\n",
    "    \n",
    "    #env.reset = types.MethodType(new_timelimit_reset, env)\n",
    "    env.env.step = types.MethodType(new_step, env.env)\n",
    "    env = normalized_env.make_normalized_env(env)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "True action space: [-1.], [1.]\n",
      "True state space: [-1.2  -0.07], [0.6  0.07]\n",
      "Filtered action space: [-1.], [1.]\n",
      "Filtered state space: [-1. -1.], [1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_id = \"MountainCarContinuous-v0\"\n",
    "env = gym.envs.make(env_id)\n",
    "env = modify_env(env)\n",
    "env._max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env_id = \"MountainCarContinuous-v0\"\n",
    "    env = gym.envs.make(env_id)\n",
    "    env = modify_env(env)\n",
    "    \n",
    "    model = deepq.models.mlp([200, 200], layer_norm=True)\n",
    "    act = deepq.learn(\n",
    "        env,\n",
    "        n_action=5,\n",
    "        q_func=model,\n",
    "        lr=1e-3,\n",
    "        max_timesteps=20000,\n",
    "        buffer_size=100000,\n",
    "        exploration_fraction=0.2,\n",
    "        exploration_final_eps=0.05,\n",
    "        print_freq=5,\n",
    "        param_noise=True,\n",
    "        target_network_update_freq=1000,\n",
    "        gamma=0.99,\n",
    "        prioritized_replay=True,\n",
    "        prioritized_replay_alpha=0.6,\n",
    "        prioritized_replay_beta0=0.9,\n",
    "        prioritized_replay_beta_iters=100000,\n",
    "        prioritized_replay_eps=1e-6\n",
    "    )\n",
    "    act.save(\"mountaincar_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "True action space: [-1.], [1.]\n",
      "True state space: [-1.2  -0.07], [0.6  0.07]\n",
      "Filtered action space: [-1.], [1.]\n",
      "Filtered state space: [-1. -1.], [1. 1.]\n",
      "============== training starts ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
