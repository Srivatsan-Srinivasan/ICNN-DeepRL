{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../src\")\n",
    "import tensorflow as tf\n",
    "\n",
    "import gym\n",
    "import baselines\n",
    "from baselines import deepq\n",
    "\n",
    "import normalized_env\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def modify_env(env):\n",
    "\n",
    "    # hack to give us control over the initial state\n",
    "    import types\n",
    "\n",
    "    def new_timelimt_step(self, action):\n",
    "        assert self._episode_started_at is not None, \"Cannot call env.step() before calling reset()\"\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        self._elapsed_steps += 1\n",
    "\n",
    "        if self._past_limit():\n",
    "            if self.metadata.get('semantics.autoreset'):\n",
    "                _ = self.reset() # automatically reset the env\n",
    "            done = True \n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def new_step(self, discrete_action):\n",
    "        \n",
    "            \n",
    "        #disc_to_cont = {\n",
    "        #    0: [-1.0],\n",
    "        #    1: [-0.5],\n",
    "        #    2: [0.0],\n",
    "        #    3: [0.5],\n",
    "        #    4: [1.0]\n",
    "        #}\n",
    "        \n",
    "        disc_to_cont = {0: [-1.0],\n",
    "         1: [-0.8],\n",
    "         2: [-0.6],\n",
    "         3: [-0.3999999999999999],\n",
    "         4: [-0.19999999999999996],\n",
    "         5: [0.0],\n",
    "         6: [0.20000000000000018],\n",
    "         7: [0.40000000000000013],\n",
    "         8: [0.6000000000000001],\n",
    "         9: [0.8],\n",
    "         10: [1.0]}\n",
    "\n",
    "        \n",
    "        #n_actions = 30\n",
    "        #disc_to_cont = {0: [-1.0],\n",
    "        # 1: [-0.9310344827586207],\n",
    "        # 2: [-0.8620689655172413],\n",
    "        # 3: [-0.7931034482758621],\n",
    "        # 4: [-0.7241379310344828],\n",
    "        # 5: [-0.6551724137931034],\n",
    "         #6: [-0.5862068965517242],\n",
    "        # 7: [-0.5172413793103449],\n",
    "        # 8: [-0.4482758620689655],\n",
    "        # 9: [-0.3793103448275862],\n",
    "        # 10: [-0.31034482758620685],\n",
    "         #11: [-0.24137931034482762],\n",
    "         #12: [-0.1724137931034483],\n",
    "         #13: [-0.10344827586206895],\n",
    "         #14: [-0.034482758620689724],\n",
    "         #15: [0.034482758620689724],\n",
    "         #16: [0.10344827586206895],\n",
    "         #17: [0.17241379310344818],\n",
    "         #18: [0.24137931034482762],\n",
    "        # 19: [0.31034482758620685],\n",
    "         #20: [0.3793103448275863],\n",
    "         #21: [0.4482758620689655],\n",
    "         #22: [0.5172413793103448],\n",
    "        # 23: [0.5862068965517242],\n",
    "        # 24: [0.6551724137931034],\n",
    "        # #25: [0.7241379310344827],\n",
    "         #26: [0.7931034482758621],\n",
    "       #  27: [0.8620689655172413],\n",
    "         #28: [0.9310344827586206],\n",
    "         #29: [1.0]}\n",
    "        \n",
    "        action = disc_to_cont[int(discrete_action[0])]\n",
    "        # hacky way to map continuous action to discrete value\n",
    "\n",
    "        position = self.state[0]\n",
    "        velocity = self.state[1]\n",
    "        force = min(max(action[0], -1.0), 1.0)\n",
    "\n",
    "        velocity += force*self.power -0.0025 * math.cos(3*position)\n",
    "        if (velocity > self.max_speed): velocity = self.max_speed\n",
    "        if (velocity < -self.max_speed): velocity = -self.max_speed\n",
    "        position += velocity\n",
    "        if (position > self.max_position): position = self.max_position\n",
    "        if (position < self.min_position): position = self.min_position\n",
    "        if (position==self.min_position and velocity<0): velocity = 0\n",
    "\n",
    "        done = bool(position >= self.goal_position)\n",
    "\n",
    "        reward = 0\n",
    "        if done:\n",
    "            reward = 100.0\n",
    "        reward-= math.pow(action[0],2)*0.1\n",
    "\n",
    "        self.state = np.array([position, velocity])\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "    # set max time step\n",
    "    \n",
    "    env._max_episode_steps = 999\n",
    "    \n",
    "    #env.reset = types.MethodType(new_timelimit_reset, env)\n",
    "    env.env.step = types.MethodType(new_step, env.env)\n",
    "    env = normalized_env.make_normalized_env(env)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.5'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "True action space: [-1.], [1.]\n",
      "True state space: [-1.2  -0.07], [0.6  0.07]\n",
      "Filtered action space: [-1.], [1.]\n",
      "Filtered state space: [-1. -1.], [1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_id = \"MountainCarContinuous-v0\"\n",
    "env = gym.envs.make(env_id)\n",
    "env = modify_env(env)\n",
    "env._max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env_id = \"MountainCarContinuous-v0\"\n",
    "    env = gym.envs.make(env_id)\n",
    "    env = modify_env(env)\n",
    "    model = deepq.models.mlp([200, 200], layer_norm=True)\n",
    "    act = deepq.learn(\n",
    "        env,\n",
    "        n_action=11,\n",
    "        q_func=model,\n",
    "        model_path=\"output/DNN/\",\n",
    "        trial_i=1,\n",
    "        lr=1e-3,\n",
    "        max_timesteps=100000,\n",
    "        buffer_size=100000,\n",
    "        exploration_fraction=0.5,\n",
    "        exploration_final_eps=0.05,\n",
    "        print_freq=5,\n",
    "        param_noise=True,\n",
    "        target_network_update_freq=1000,\n",
    "        gamma=0.99,\n",
    "        prioritized_replay=True,\n",
    "        prioritized_replay_alpha=0.6,\n",
    "        prioritized_replay_beta0=0.9,\n",
    "        prioritized_replay_beta_iters=100000,\n",
    "        prioritized_replay_eps=1e-6\n",
    "    )\n",
    "    act.save(\"mountaincar_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "True action space: [-1.], [1.]\n",
      "True state space: [-1.2  -0.07], [0.6  0.07]\n",
      "Filtered action space: [-1.], [1.]\n",
      "Filtered state space: [-1. -1.], [1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srivatsan/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/srivatsan/.local/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 92       |\n",
      "| episodes                | 5        |\n",
      "| mean 100 episode reward | -64      |\n",
      "| steps                   | 3995     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 10       |\n",
      "| mean 100 episode reward | -64      |\n",
      "| steps                   | 8990     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 73       |\n",
      "| episodes                | 15       |\n",
      "| mean 100 episode reward | -66.5    |\n",
      "| steps                   | 13985    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 63       |\n",
      "| episodes                | 20       |\n",
      "| mean 100 episode reward | -65.8    |\n",
      "| steps                   | 18980    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 54       |\n",
      "| episodes                | 25       |\n",
      "| mean 100 episode reward | -66      |\n",
      "| steps                   | 23975    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 44       |\n",
      "| episodes                | 30       |\n",
      "| mean 100 episode reward | -67.3    |\n",
      "| steps                   | 28970    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 35       |\n",
      "| episodes                | 35       |\n",
      "| mean 100 episode reward | -67.8    |\n",
      "| steps                   | 33965    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 25       |\n",
      "| episodes                | 40       |\n",
      "| mean 100 episode reward | -67.3    |\n",
      "| steps                   | 38960    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 16       |\n",
      "| episodes                | 45       |\n",
      "| mean 100 episode reward | -66.9    |\n",
      "| steps                   | 43955    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 6        |\n",
      "| episodes                | 50       |\n",
      "| mean 100 episode reward | -66.6    |\n",
      "| steps                   | 48950    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 55       |\n",
      "| mean 100 episode reward | -67.1    |\n",
      "| steps                   | 53945    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 60       |\n",
      "| mean 100 episode reward | -66.8    |\n",
      "| steps                   | 58940    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 65       |\n",
      "| mean 100 episode reward | -67.1    |\n",
      "| steps                   | 63935    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 70       |\n",
      "| mean 100 episode reward | -66.9    |\n",
      "| steps                   | 68930    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 75       |\n",
      "| mean 100 episode reward | -67.2    |\n",
      "| steps                   | 73925    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 80       |\n",
      "| mean 100 episode reward | -67.4    |\n",
      "| steps                   | 78920    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 85       |\n",
      "| mean 100 episode reward | -67.2    |\n",
      "| steps                   | 83915    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 90       |\n",
      "| mean 100 episode reward | -67      |\n",
      "| steps                   | 88910    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 95       |\n",
      "| mean 100 episode reward | -67.3    |\n",
      "| steps                   | 93905    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | -67.1    |\n",
      "| steps                   | 98900    |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
